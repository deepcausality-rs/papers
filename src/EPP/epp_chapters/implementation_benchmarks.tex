\subsection{Benchmarks}
\label{sec:implementation_benchmarks}

\subsubsection{UltraGraph Benchmarks}

All benchmarks were completed on a 2023 Macbook Pro with a M3 Max CPU.

\textbf{Dynamic Graph Benchmarks:}

The dynamic graph structure, when the graph is in an unfrozen state, is optimized for efficient mutation. The table below summarizes the performance characteristics of the key operations.

% In your preamble, make sure you have: \usepackage{booktabs}
\begin{table}[h!]
\centering
\caption{Dynamic Graph Mutation Performance}
\label{tab:dynamic-graph-perf}
\begin{tabular}{lrlrr}
\toprule
\textbf{Benchmark Name} & \textbf{Graph Size} & \textbf{Operation} & \textbf{Estimated Time (Median)} & \textbf{Outliers Detected} \\
\midrule
\texttt{small\_add\_node}  & 10    & \texttt{add\_node} & 29.099 ns & 14\% (14 / 100) \\
\texttt{medium\_add\_node} & 100   & \texttt{add\_node} & 45.864 ns & 12\% (12 / 100) \\
\texttt{large\_add\_node}  & 1,000 & \texttt{add\_node} & 39.293 ns & 11\% (11 / 100) \\
\texttt{small\_get\_node}  & 10    & \texttt{get\_node} & 3.9417 ns & 8\% (8 / 100)  \\
\texttt{medium\_get\_node} & 100   & \texttt{get\_node} & 3.9849 ns & 2\% (2 / 100)  \\
\texttt{large\_get\_node}  & 1,000 & \texttt{get\_node} & 3.9916 ns & 7\% (7 / 100)  \\
\bottomrule
\end{tabular}
\end{table}

Benchmark source code is available in the UltrGraph Github repository\footnote{\url{https://github.com/deepcausality-rs/deep_causality/tree/main/ultragraph/benches}}.


\textbf{Static Graph Benchmarks:}

The new architecture causes the largest and most significant performance gains for algorithms running over large graphs (100k or more nodes) because of its close alignment with contemporary hardware.
By combining an instantaneous O(1) lookup with a perfectly linear scan over a node's neighbors, Ultragraph creates the ideal scenario for the CPU's prefetcher to easily anticipates a straight-line sprint through memory. The result becomes more notable the more data the prefetcher can load ahead of time, thus the disproportional performance gains on larger graphs.

% In your preamble, make sure you have: \usepackage{booktabs}
\begin{table}[h!]
\centering
\caption{Memory Usage and Scaling Performance on a Linear Graph}
\label{tab:memory-scaling}
\begin{tabular}{lrrrr}
\toprule
\textbf{Number of Nodes} & \textbf{Memory Usage} & \textbf{\texttt{eval\_subgraph}} & \textbf{\texttt{eval\_path}} & \textbf{\texttt{eval\_cause}} \\
\midrule
\textbf{100,000}         & 55 MB                 & 0.68 ms                         & 0.57 ms                      & \textbf{5.4 ns}               \\
\textbf{1,000,000}       & 350 MB                & 11.12 ms                        & 6.95 ms                      & \textbf{5.5 ns}               \\
\textbf{10,000,000}      & 3 GB                  & 114 ms                          & 85.80 ms                     & \textbf{5.6 ns}               \\
\textbf{100,000,000}     & 32 GB                 & 1.23 s                          & 0.98 s                       & \textbf{5.5 ns}               \\
\bottomrule
\end{tabular}
\end{table}

Benchmark source code is available in the DeepCausality Github repository\footnote{\url{https://github.com/deepcausality-rs/deep_causality/tree/main/deep_causality/benches}}.

\textbf{Observations}:

\textbf{Constant Time to get a single node:} The benchmark evaluate\_single\_cause returns always takes about 5.5. ns regardless of
 whether the node lookup happens before or during the benchmark loop and regardless of whether blackbox is used or not. 
 The time does not change with the size of the graph because the implementation of the underlying get\_node is just two O(1) array lookup 
 to find the index and than a straight redirect to a virtual memory address, which in this case, is close to the physical limit of the hardware memory architecture. 


\textbf{Near-Linear Scalability:} Both the memory usage and the execution time for the subgraph and shortest\_path tasks appear to scale in a roughly linear fashion with the number of nodes. 
A 10x increase in nodes results in a roughly 10x increase in time and memory. 

\textbf{All benchmarks are single-threaded.} The performance shown in all benchmarks is from a single core. Initial experiments showed that for graphs up to 1 million nodes, the overhead of even highly-optimized parallel libraries like rayon resulted in a  net performance loss of 30\% or more compared to the single-threaded version. This is a testament to the extreme  efficiency of the CSR layout when paired with modern CPU caches and prefetchers.  The results suggest that meaningful gains from concurrency will only appear on very large graphs (likely 10M-50M nodes  and above). However, this requires a concurrency model carefully designed to avoid the cache-invalidation issues common in work-stealing schedulers (used by rayon and Tokio). 

% \subsubsection{DeepCausality Benchmarks}


\newpage