\section{Related Work}
\label{sec:related_work}

The study of causality and causal inference aims to distinguish genuine cause-and-effect relationships from mere associations. Traditionally, establishing causality often relied on carefully controlled randomized controlled trials. However, significant theoretical advancements have shown that causal knowledge can be inferred from observational data by examining patterns of conditional independence among variables, given explicit assumptions \cite{pearl2018theoretical}.



\subsection{Foundational Theories of Causal Inference}
\label{subsec:foundational_theories}

The endeavor to formalize and compute causal relationships draws upon several influential theoretical frameworks. Understanding these foundations is crucial for situating contemporary advancements and appreciating the nuances of different approaches to causal reasoning.

\subsubsection{Directed Acyclic Graphs (DAGs)}

A foundational framework for representing causal structures is based on graphical causal models, most notably Directed Acyclic Graphs (DAGs) \cite{verma1986causal, pearl1988probabilistic, Glymour2019Review, koller2009probabilistic}. In these models, variables are typically represented by nodes, and directed edges indicate direct causal influences \cite{pearl1988probabilistic}. The impact of interventions, conceptualized by operators like the \texttt{do}-operator which sets a variable's value independently of its usual causes, can be analyzed within this framework to predict outcomes under hypothetical scenarios \cite{pearl1988probabilistic, Pearl2009Causality}. The theoretical underpinnings of Structural Causal Models (SCMs), which are closely related to graphical models, have been extensively studied \cite{pearl2000causality, Peters2017Elements, bareinboim2020causal, janzing2016algorithmic, Peters2022Causal}. Methods exist for handling complex scenarios, including incorporating latent variables \cite{Mohan2021Graphical, richardson2003causal} and understanding the relationship between different causal models \cite{Verma1990Equivalence, pearl2018theoretical}. Policy interventions in specific graphical structures, such as Lauritzen-Wermuth-Freydenburg (LWF) latent-variable chain graphs, have also been investigated \cite{sherman2020general}. This includes work providing a novel identification result for effects of policy interventions in these graphs \cite{sherman2020general}.

\subsubsection{Structural Causal Model (SCM)}

The dominant paradigm in modern computational causality is arguably the Structural Causal Model (SCM) framework, extensively developed by Judea Pearl and his colleagues \cite{Pearl2009Causality}. An SCM consists of a set of variables, some of which are designated as exogenous (external, uncaused within the model) and others as endogenous (their values are determined by other variables within the model). The relationships between these variables are represented by a set of structural equations, typically of the form $X_i = f_i(\mathbf{PA}_i, U_i)$, where $\mathbf{PA}_i$ are the direct causal parents of $X_i$ in the associated causal graph, and $U_i$ are exogenous error terms representing unmodeled influences or inherent stochasticity. These structural equations are considered to represent autonomous, invariant causal mechanisms. Graphically, SCMs are typically depicted using {Directed Acyclic Graphs (DAGs), where nodes represent variables and directed edges $X_j \to X_i$ indicate that $X_j$ is a direct cause of $X_i$ (i.e., $X_j \in \mathbf{PA}_i$). This graphical representation provides an intuitive way to encode causal assumptions and to determine statistical independencies via the criterion of \textit{d-separation}.

A cornerstone of Pearl's framework is the \textit{do-calculus}, a set of three axiomatic rules that allows for the inference of the effects of interventions from a combination of observational data and the causal graph structure, even when direct experimentation is not possible \cite{Pearl2009Causality}. An intervention, denoted $do(X_j=x_j')$, represents an external manipulation that sets the variable $X_j$ to a specific value $x_j'$, thereby severing the links from its original parents $\mathbf{PA}_j$ and altering the system's natural dynamics. The ability to calculate post-intervention distributions, $P(Y | do(X=x))$, is central to predicting the consequences of actions and policies. Bayesian Networks, which are DAGs coupled with conditional probability distributions $P(X_i | \mathbf{PA}_i)$, are closely related to SCMs and are often used to represent the observational probability distribution $P(\mathbf{X})$ entailed by an SCM under specific assumptions about the error terms $U_i$. They provide a powerful tool for probabilistic inference under passive observation, but require the \textit{do-calculus} or similar interventional logic to reason about causal effects.

\newpage

\subsubsection{Counterfactuals: What if?}
\label{subsec:counterfactuals}

While discovering causal structures and predicting the effects of interventions (``What if we do $X=x$?'') are fundamental tasks in causal inference, the ability to compute counterfactual queries represents a deeper and often more insightful level of causal reasoning \cite{Pearl2009Causality}. Counterfactuals address questions about alternative realities or "what might have been" (``What if $X$ had been $x'$, given that we observed $X=x$ and $Y=y$?''). This form of reasoning is crucial for tasks such as understanding individual responsibility, learning from past mistakes, diagnosing failures, and fine-tuning policies. It requires moving beyond population-level effects of interventions to consider specific individuals or units in specific factual circumstances \cite{Pearl2009Causality, Balke1994Probabilistic}.

Within Pearl's Structural Causal Model (SCM) framework, computing a counterfactual, denoted as $Y_x(u)$ (the value $Y$ would have taken in unit $u$ had $X$ been $x$), involves a three-step algorithmic process \cite{Pearl2009Causality}:
\begin{enumerate}
    \item \textbf{Abduction:} Use the available factual evidence (e.g., observed values of some variables) to update the probability distribution over the exogenous variables $U$. This step accounts for the specific unit or situation under consideration by inferring the background conditions consistent with the observed facts.
    \item \textbf{Action:} Modify the original SCM by replacing the structural equation for the counterfactual antecedent $X$ with $X=x'$ (the hypothetical condition), effectively performing a "mini-surgery" on the model as in the \textit{do}-calculus. The equations for other variables remain unchanged, reflecting the principle that interventions only alter the targeted mechanism directly.
    \item \textbf{Prediction:} Compute the probability of the counterfactual consequent $Y$ using the modified model and the updated distribution of $U$ (from the abduction step). This yields the probability $P(Y_{x'} = y' | \text{evidence})$.
\end{enumerate}

This process allows for a principled way to reason about hypothetical scenarios that differ from what was actually observed, effectively comparing parallel possible worlds \cite{Pearl2009Causality, Morgan2015Counterfactuals}. Foundational work also explored the bounding and identification of specific types of counterfactual queries related to probabilities of causation \cite{tian2000probabilities}. Formal systems like Pearl's \textit{do}-calculus provide tools for determining if causal effects under intervention are identifiable from observational data \cite{tian2002identification}, and algorithms exist to automate this process \cite{shpitser2012efficient}, which are often prerequisite steps before full counterfactual queries can be comprehensively addressed.

The extension of these concepts to practical applications and more complex settings remains an active area of research. For instance, model-agnostic approaches aim to enable counterfactual reasoning without full specification of the SCM, particularly in dynamic environments where systems evolve over time. Furthermore, the domain of causal bandits, which focuses on online decision-making and learning under uncertainty, increasingly incorporates causal background knowledge and aspects of counterfactual reasoning to optimize sequences of actions and learn policies more efficiently than purely correlational reinforcement learning approaches \cite{Lattimore2016Causal, Lee2018Structural, Zhang2022Causal, Bilodeau2022Adaptively}. The capacity for counterfactual reasoning thus forms a critical component of advanced intelligent systems that can not only predict and act, but also reflect, learn, and adapt based on a deep understanding of cause and effect in alternative scenarios.

More recent work explores model-agnostic approaches to counterfactual reasoning, particularly in dynamic environments \cite{Berrevoets2021ModelAgnostic}, and investigates optimizing treatment effects in such settings \cite{Berrevoets2022Treatment}. Causal bandits also incorporate causal background knowledge into online decision-making problems \cite{Lattimore2016Causal, Lee2018Structural, Zhang2022Causal, Bilodeau2022Adaptively}.


\subsubsection{Potential Outcomes}
\label{subsec:potential_outcomes}

Alongside SCMs, Potential Outcomes Framework, also known as the Rubin Causal Model (RCM) \cite{holland1986statistics}, offers another rigorous foundation for causal inference, with early conceptualizations by Neyman \cite{splawa1990application} and formally developed for observational studies by Rubin \cite{rubin1974estimating}. It has been particularly influential in statistics, econometrics, and the social sciences. This framework defines the causal effect of a treatment (or exposure) on an individual unit by considering the potential outcomes that unit would exhibit under different treatment assignments. For a binary treatment $T \in \{0,1\}$, each unit $i$ is conceptualized as having two potential outcomes: $Y_i(1)$, the outcome if unit $i$ receives the treatment, and $Y_i(0)$, the outcome if unit $i$ receives the control. The individual treatment effect (ITE) is then $Y_i(1) - Y_i(0)$. A core challenge, often termed the "fundamental problem of causal inference," is that only one of these potential outcomes can be observed for any given unit \cite{holland1986statistics}.

Inference in this framework hinges on crucial assumptions, such as the Stable Unit Treatment Value Assumption (SUTVA), which posits no interference between units and well-defined treatment versions. When all confounders are believed to be observed, the key assumption is \textbf{ignorability} (or unconfoundedness), which states that treatment assignment is independent of potential outcomes, conditional on the observed covariates \cite{rosenbaum1983central}. Under this assumption, causal effects can be estimated using methods like matching, stratification, or inverse probability weighting based on propensity scores \cite{rosenbaum1983central}.

In many settings, the belief that all confounders have been measured is not plausible. To address this, applied researchers have developed a powerful toolkit of \textbf{quasi-experimental methods} that can enable causal identification even in the presence of unobserved confounding. These methods are cornerstones of modern computational causality and include:
\begin{itemize}
    \item \textbf{Instrumental Variables (IV):} This technique is used to handle unobserved confounding by leveraging an "instrument": a variable that is correlated with the treatment but is not causally related to the outcome except through its effect on the treatment. The instrument provides a source of exogenous variation in the treatment, allowing for the estimation of a causal effect that is not biased by the unobserved confounders \cite{angrist1996identification}.
    \item \textbf{Difference-in-Differences (DiD):} Leveraging panel data (observations of the same units over time), DiD estimates the effect of a treatment by comparing the change in the outcome for a treated group before and after an intervention to the change in the outcome for an untreated group over the same time period. This method controls for unobserved confounders that are constant over time by differencing them out\cite{card1994minimum}\cite{callaway2021difference}.
    \item \textbf{Regression Discontinuity Design (RDD):} RDD is applicable when the treatment is assigned based on a sharp cutoff in a continuous variable (the "running variable"). By comparing the outcomes of units just below the cutoff to those just above it, RDD can provide an unbiased estimate of the local causal effect at the threshold, mimicking a randomized experiment in a narrow window around the cutoff \cite{imbens2008regression}.
\end{itemize}

While SCMs provide an explicit language for encoding causal mechanisms, the Potential Outcomes framework, supported by these robust quasi-experimental methods, provides a powerful applied toolkit for estimating causal effects from complex observational data. 

\subsection{Invariant Prediction and Out-of-Distribution Generalization}

A central challenge in modern machine learning remains robust out-of-distribution (OOD) generalization, as models trained under the standard I.I.D. assumption often fail when deployed in new or shifting environments. A powerful approach to this problem is rooted in the causal principle of invariance: the idea that while statistical correlations can be spurious and brittle, true causal mechanisms remain stable across different contexts \cite{pearl2000causality}.

This principle has been operationalized into a formal framework for both causal discovery and robust prediction. The seminal work in this area is Invariant Causal Prediction (ICP), developed by Peters, Bühlmann, and Meinshausen \cite{peters2016invariant}. The ICP framework leverages data from multiple distinct "environments" or "settings." It posits that a set of variables constitutes the direct causes of a target if and only if the conditional distribution of the target given those variables remains invariant across all environments. By searching for a set of predictors that yields such a stable predictive model, ICP can identify causal relationships and produce a model that is robust to the types of distributional shifts observed during training.

This idea has been influential in the deep learning community, inspiring methods aimed at learning invariant representations. A prominent example is Invariant Risk Minimization (IRM), which seeks to learn a data representation such that the optimal classifier on top of that representation is the same for every training environment \cite{arjovsky2019invariant}. The goal is to isolate invariant causal features from spurious, environment-specific correlations. Other related approaches, such as Risk Extrapolation (REx) \cite{krueger2021out}, also aim to improve OOD performance by enforcing penalties on models whose performance is unstable across environments. Collectively, this body of work formalizes the intuition that a model based on causal structure should generalize better than one that merely interpolates the training data, representing a major school of thought in building more reliable and robust machine learning systems.

\newpage

\subsection{Causal Discovery}

A central task in the field is causal discovery, which focuses on learning the causal structure, represented by the graph, from observed data alone. A comprehensive survey categorizes existing methods for causal discovery on both independent and identically distributed (I.I.D.) data and time series data, including approaches for both types of data. According to this survey, categories include Constraint-based, Score-based, FCM-based, Hybrid-based, Continuous-Optimization-based, or Prior-Knowledge-based. Constraint-based methods infer relationships by testing for conditional independencies in the data \cite{Glymour2019Review, Spirtes2000Causation, Eberhardt2017Introduction}. Score-based methods search over potential graph structures and evaluate them based on how well they fit the data, often including a penalty for complexity \cite{Chickering2002Optimal}. The KGS method \cite{hasan2022kcrl}, for example, leverages prior causal information such as the presence or absence of a causal edge to guide a greedy score-based causal discovery process towards a more restricted and accurate search space. It demonstrates how incorporating different types of edge constraints can enhance both accuracy and runtime for graph discovery and candidate scoring, concluding that any type of edge information is useful. This method relates to the KCRL framework \cite{hasan2022kcrl}. Continuous optimization techniques formulate causal discovery as an optimization problem, potentially involving differentiable approaches that can handle constraints like acyclicity. The NOTEARS framework is one such example \cite{Zheng2018Dags}, and studies have analyzed its performance and proposed post-processing algorithms to enhance its precision and efficiency. A study provides an in-depth analysis of the NOTEARS framework for causal structure learning, proposing a local search post-processing algorithm that significantly increased the precision of NOTEARS and other algorithms \cite{hasan2023analysis}.


\subsection{Causal Inference and Discovery for (Hyper)graphs}
\label{subsec:causal_graphs_hypergraphs}

The representation of causal relationships via graphical models, predominantly Directed Acyclic Graphs (DAGs) as foundational to Structural Causal Models (SCMs) \cite{Pearl2009Causality}, is a cornerstone of computational causality. Much research has focused on discovering these graph structures from observational or interventional data (causal discovery) and subsequently estimating causal effects based on the identified graph (causal inference). While traditional methods often assume simpler pairwise relationships, the inherent complexity of many real-world systems necessitates considering more intricate relational structures.

Recent work has begun to explicitly tackle causal inference in settings involving multi-way interactions best represented by hypergraphs. Ma et al. \cite{ma2022learning} directly address the problem of estimating Individual Treatment Effects (ITE) on hypergraphs, specifically accounting for high-order interference where group interactions (modeled by hyperedges) influence individual outcomes. Their proposed HyperSCI framework leverages hypergraph neural networks to model these spillover effects and uses representation learning to control for confounders, demonstrating the utility of explicitly considering hypergraph topology for ITE estimation from observational data. This represents a significant step beyond assuming only pairwise interference, which is common in ordinary graph-based causal inference. While the work by Ma et al. focuses on statistical ITE estimation on a \textit{given} hypergraph, it highlights the increasing recognition of hypergraph structures as vital for certain causal problems.

The broader field of graph mining and network science also provides a rich backdrop, with techniques for link prediction and understanding influence spread, though these often operate at a correlational level rather than a strictly causal one. The challenge remains to bridge network science concepts with formal causal reasoning in these complex relational systems.


\subsection{Causal Inference for Time Series}
\label{subsec:causal_timeseries}

Causal inference for time series data introduces a unique set of challenges and opportunities compared to static, cross-sectional settings. The inherent temporal ordering of observations provides strong, intuitive information about potential causal directionality---causes generally precede their effects---but also necessitates methods that can handle auto-correlation, non-stationarity, feedback loops, and varying time lags in causal influences.

A foundational concept in this domain is Granger Causality, originally developed by Clive Granger for economic time series \cite{Granger1969Investigating, granger1980testing}. A time series $X_t$ is said to Granger-cause another time series $Y_t$ if past values of $X_t$ contain information that helps predict future values of $Y_t$ beyond the information already contained in past values of $Y_t$ itself. This is typically tested using vector autoregression (VAR) models and statistical tests on the coefficients of lagged variables \cite{geweke1982measurement, padav2021granger}. While widely applied, standard Granger causality is primarily about predictive improvement and may not always align with true mechanistic causation, especially in the presence of unobserved confounders, instantaneous effects, or non-linear relationships. Extensions and refinements have been developed to address some of these limitations, including non-linear Granger causality tests and methods incorporating multivariate information criteria.

To explicitly model evolving causal relationships and dependencies over time, dynamic graphical models have been developed. The Dynamic Uncertain Causality Graph (DUCG) \cite{Zhang2012Dynamic} is one such framework, specifically designed to represent and reason about causal relationships that themselves change as a system evolves. DUCGs find applications in complex dynamic systems, such as fault diagnosis in nuclear power plants where understanding the temporal progression of component failures is critical \cite{Deng2018Cubic, Hu2017Accident}. These models often aim to unify diagnostic reasoning (what caused an observed state?) with treatment or control strategies (what intervention will lead to a desired future state?) \cite{Deng2020Towards}.

More recently, deep learning techniques have been increasingly applied to causal discovery and inference in time series. For example, the Time-Series Causal Discovery Framework (TCDF) utilizes attention-based convolutional neural networks to learn causal relationships, explicitly trying to identify relevant time lags and dependencies \cite{Nauta2019Causal}. Research in this direction often focuses on challenges such as optimizing hyperparameters for these complex models, ensuring robustness to varying noise levels and non-stationarities in the data, improving the interpretability of attention mechanisms to understand which past events are deemed causally salient, and developing robust causal validation methods beyond simple predictive accuracy. Other important research avenues in time series causality include:
\begin{itemize}
    \item \textbf{Handling Unobserved Confounders:} Just as in static settings, unobserved common causes can induce spurious relationships between time series. Methods that attempt to detect or adjust for such confounding, perhaps using instrumental variable approaches adapted for time series or by searching for specific types of conditional independencies, are crucial.
    \item \textbf{State-Space Models and Causal Inference:} Integrating causal concepts with state-space models (e.g., Kalman filters and their non-linear extensions) allows for reasoning about causality between latent (unobserved) states as well as observed variables.
    \item \textbf{Interventional Time Series Analysis:} Developing methods to estimate the effect of specific interventions applied at certain points in time on the future trajectory of one or more time series. This is vital for policy evaluation and system control.
    \item \textbf{Causal Discovery from Irregularly Sampled or High-Dimensional Time Series:} Many real-world time series (e.g., medical patient data, sensor networks) are not regularly sampled or involve a very large number of variables, posing challenges for traditional methods.
    \item \textbf{Information-Theoretic Approaches:} Methods like Transfer Entropy [Schreiber, 2000, \textit{Measuring information transfer}] provide a non-parametric way to quantify directed information flow between time series, offering an alternative perspective to Granger causality, especially for detecting non-linear interactions.
\end{itemize}

The temporal dimension thus adds significant complexity but also provides a powerful constraint (time ordering) that can be leveraged for causal reasoning, making this a vibrant and critical area of ongoing research.


\subsection{The Role of Context in Causal Inference}
\label{subsec:context_temporality_causality}

While foundational causal frameworks like SCMs implicitly allow for conditioning variables, the explicit, structured, and dynamic modeling of \textit{context} as a multi-faceted entity is a growing area of focus, crucial for applying causal inference to complex, real-world systems. The Jiao et al. survey \cite{jiao2024causal} highlights numerous deep learning applications where contextual understanding is paramount, from visual commonsense reasoning to multimodal interactions, and notes the challenges posed by contextual shifts and confounders.

Berrevoets et al. \cite{berrevoets2022navigatingcausaldeeplearning, berrevoets2024causaldeeplearning} propose a conceptual ``map of causal deep learning'' (CDL) that explicitly incorporates dimensions for structural knowledge, parametric assumptions, and significantly, a temporal dimension. They argue that time is not merely another variable but introduces unique considerations in causal settings, such as the fundamental principle that causes precede effects, and the potential for feedback loops or evolving relationships in dynamic systems. Their framework aims to help researchers and practitioners categorize CDL methods based on how they handle these dimensions, including whether they operate on static data or explicitly model temporal dynamics. For instance, they differentiate models based on whether they assume ``no structure,'' ``plausible causal structures'' (often derived from statistical independencies), or a ``full causal structure'' as input, and similarly categorize parametric assumptions from non-parametric to fully known factors. The temporal axis distinguishes between static models and those designed for time-series data where variables are observed repeatedly. While this work by Berrevoets et al. primarily offers a \textit{taxonomy and conceptual guide} for the emerging field of CDL rather than a specific implemented reasoning engine, it underscores the increasing recognition of structured context, and especially temporality, as a first-class concern in bridging deep learning with robust causal inference.
This emphasis on temporal context aligns with established work in time series causality, such as Granger causality \cite{Granger1969Investigating} and dynamic graphical models like DUCGs \cite{Zhang2012Dynamic}, which inherently focus on how relationships evolve over time. However, modern approaches, including those at the intersection of deep learning and causality, seek richer representations of temporal context beyond simple lagged variables. For example, methods like TCDF \cite{Nauta2019Causal} attempt to learn relevant temporal dependencies and attention patterns. The challenge remains to develop frameworks that can uniformly reason over diverse types of contextual information (static attributes, explicit temporal sequences, spatial relationships (both Euclidean and non-Euclidean), and even abstract conceptual states) and integrate this rich contextual understanding directly into the causal reasoning process. The ability to model multiple, potentially interacting contexts, and to allow these contexts to be dynamically updated, is key to building causal AI systems that can adapt to real-world complexities.



\subsection{Hierarchical Causality}
\label{subsec:hierarchical_causality}

A significant frontier in causal inference is its extension to handle hierarchical or nested data structures, a common feature in fields from social science to biology. While hierarchical Bayesian modeling is a standard statistical tool for such data, causal modeling has traditionally forced a difficult choice: either aggregate the data to the unit level, losing valuable information, or ignore the group structure, risking incorrect inferences.
Recent work by Weinstein and Blei\cite{weinstein2024hierarchical} formalizes this problem by introducing Hierarchical Causal Models (HCMs). They extend Structural Causal Models (SCMs) by incorporating the concept of plates from graphical modeling to explicitly represent the nested structure. The central and powerful insight of their work is that disaggregating data and modeling the hierarchy can enable causal identification even in situations where it would be impossible with "flat" or aggregated data. For instance, with an unobserved unit-level confounder (e.g., a school's budget), the within-unit variation (e.g., student-level randomization) provides a "natural experiment" that can be leveraged to control for the confounder.
To operationalize Hierarchical Causal Models, they develop a systematic, nonparametric identification procedure that extends the do-calculus. The HCM framework provides a formal causal justification for many existing methods, such as fixed-effects and difference-in-difference models, which can be seen as specific parametric instances of an HCM. Their work provides a broad and rigorous toolkit for analyzing cause and effect in multi-level systems, formally connecting the principles of hierarchical modeling with the inferential power of graphical causal models.

\subsection{(Geometric) Deep Learning for Causal Inference and Representation}
\label{subsec:geometric_dl_causality}

Deep learning has achieved remarkable success in various tasks,such as neural architecture search \cite{Baker2017aDesigning, Bender2018Understanding} and techniques for handling complex relationships in data, such as those explored using hypergraphs \cite{Ouvrard2020Hypergraphs, Berge1973Graphs}. Hypergraphs, introduced by Berge in 1973 \cite{Berge1973Graphs}, can model multi-way relationships and have found applications in areas like visualization \cite{Alsallakh2016State, Jacomy2014ForceAtlas2}, partitioning \cite{Catalyurek1999Hypergraph, Devine2006Parallel, Yang2017Hypergraph}, and recommender systems \cite{Zheng2018Novel, Zhou2007Learning, Wu2018Nonnegative, Jin2015Low, Zhu2015ContentBased, Zhu2016Heterogeneous}. Link prediction, particularly in multiplex networks, is another active area where deep learning is applied \cite{Potluru2020Deeplex, Zhang2018Link}.

The intersection of deep learning with causal inference is a rapidly expanding research area, aiming to leverage the expressive power of neural networks to address challenges in causal representation learning, discovery, and effect estimation \cite{deng2022deep, jiao2024causal}. Many approaches focus on adapting deep learning architectures to better estimate treatment effects from observational data, often by learning balanced representations of covariates to mitigate confounding bias or by modeling complex response surfaces. Ramachandra \cite{ramachandra2018deep} proposes the use of deep autoencoders for generalized neighbor matching to estimate ITE, focusing on dimensionality reduction while preserving local neighborhood structure, and also suggests using Deep Neural Networks (DNNs) for improved propensity score estimation (PropensityNet). These methods exemplify the application of standard deep learning architectures to enhance specific statistical tasks within the potential outcomes framework, typically under assumptions such as the Stable Unit Treatment Value Assumption (SUTVA), which precludes interference.


A notable direction involves incorporating prior causal knowledge into deep generative models, enabling the generation of data that respects a given causal graph. While combining causal discovery with generative modeling is a goal, these methods are often constrained by the fundamental limitations of causal discovery \cite{Glymour2019Review}. Specific efforts include incorporating causal graphical prior knowledge into predictive modeling \cite{Teshima2021Incorporating} and matching learned causal effects with domain priors in neural networks \cite{Kancheti2022Matching}. Applications in finance have also utilized informed machine learning frameworks based on a priori causal graphs for prediction tasks. The area of causal reinforcement learning and causal bandits also represents significant related work in combining causality with learning agents that interact with environments \cite{Lattimore2016Causal, Lee2018Structural, Zhang2022Causal, Bilodeau2022Adaptively, Lu2020Regret, Tennenholtz2021Bandits}.
	
More fundamentally, researchers are exploring how geometric deep learning principles can inform the design of causal models capable of handling complex data structures and respecting informational constraints. Acciaio et al. \cite{acciaio2024designing} introduce a ``universal causal geometric DL framework,'' featuring the Geometric Hypertransformer (GHT). Their work is concerned with the universal approximation of causal maps between discrete-time path spaces, which may be non-Euclidean metric spaces such as Wasserstein spaces, while strictly respecting the forward flow of information inherent in causal processes. The GHT employs hypernetworks to adapt its parameters over time and aims to provide theoretical guarantees for approximating Hölder continuous functions between these complex spaces. Although highly theoretical and with a focus on applications in stochastic analysis and mathematical finance, this line of work signifies a deep engagement with geometric structures, non-Euclidean spaces, and transformer-like attention mechanisms within a causal learning context. Their ``geometric attention mechanism'' operating on Quantizable and Approximately Simplicial (QAS) spaces represents a sophisticated approach to handling non-Euclidean output geometries. A survey by Jiao et al. \cite{jiao2024causal} details various methods where deep learning is applied to causal discovery (e.g., leveraging neural networks for GraN-DAG or extensions of NOTEARS) or to augment specific causal inference tasks within existing deep learning modalities (e.g., developing causal attention mechanisms in computer vision, or applying causal methods to Graph Neural Networks). This body of work collectively seeks to imbue deep learning models with a degree of causal awareness or to use their representational power to overcome limitations in traditional causal inference techniques. The ongoing challenge is to move beyond enhancing specific sub-tasks towards building more integrated and principled frameworks for comprehensive causal reasoning using deep learning.

\subsection{Causal Representation Learning}

Causal Representation Learning (CRL) has emerged as a field dedicated to this problem, aiming to learn latent representations that are not just statistically useful but are also causally meaningful \cite{Scholkopf2021Toward}. The central goal is to learn a mapping from high-dimensional observations to a latent space where the dimensions correspond to independent, underlying causal factors.
Early efforts in deep learning focused on learning "disentangled" representations, with the hope that unsupervised methods could automatically separate data into its factors of variation. However, foundational work by Locatello et al. demonstrated that learning disentangled representations without inductive biases is theoretically impossible \cite{Locatello2019Challenging}. This finding has spurred the CRL community to propose that causal structure is the necessary inductive bias to achieve meaningful and identifiable disentanglement. The core assumption is that real-world changes often arise from interventions on a sparse set of high-level causal mechanisms. A model that captures these mechanisms in its latent space should therefore be more robust and generalize better out-of-distribution. A key technical challenge in this area is the identifiability of the latent causal variables—ensuring that the learned representation is unique and corresponds to the true underlying causal structure. 

\subsection{The Causaloid Framework}

A significant effort towards establishing a framework for probabilistic theories with dynamic causal structure was presented by Hardy \cite{hardy2005probability}. Hardy proposes a framework aimed at unifying quantum theory (QT) and general relativity (GR) as a step towards quantum gravity (QG). The core of this unification lies in a generalized theory of causality, capable of describing both the probabilistic nature and fixed causal structure of QT, as well as the deterministic nature and dynamic causal structure of GR, within a single formalism. The core of this new framework of unified causality is the "causaloid," a mathematical object designed to encapsulate all information about the causal relationships within a physical system.
The framework begins from an operational standpoint, focusing on "recorded data" which consists of "actions" and "observations" associated with "elementary regions" of spacetime.
The causaloid itself is a theory-specific mathematical entity, primarily represented by a collection of "lambda matrices" ($\Lambda$). These matrices quantify how the complexity of describing a composite region (specifically, the number of fiducial measurements needed to determine its state) is reduced due to causal connections between its component elementary regions. Associated with any region $R$ and an experimental procedure $F_R$ resulting in outcome $X_R$ are "r-vectors," denoted $r(X_R, F_R)(R)$, which are analogous to operators in QT \cite{hardy2005probability}. A key innovation is the "causaloid product," which is governed by the causaloid (via the lambda matrices). This product combines r-vectors of sub-regions to form the r-vector for a composite region, e.g., $r(R_1 \cup R_2) = r(R_1) \hat{\;} r(R_2)$. This product aims to unify the different ways systems are composed in QT, such as tensor products for space-like separated systems and sequential (matrix) products for timelike evolutions. Probabilities for joint outcomes, conditioned on experimental settings, are then derived from these r-vectors. A crucial feature of the causaloid formalism is that it does not impose a fixed causal structure or a background time a priori. Instead, the causal relations are implicitly defined by the causaloid itself. Hardy demonstrated how both classical probability theory and quantum theory can be cast within this framework, with the differences between theories being encoded entirely in the specification of their respective causaloids. The ultimate aim is to provide a structure wherein the dynamic causal aspects of GR can be consistently combined with the probabilistic nature of QT. Hardy also introduces "causaloid diagrams" as a visual tool to represent and compute the causaloid based on local lambda matrices for nodes (elementary regions) and links (pairwise connections), particularly under simplifying assumptions met by QT and classical probability \cite{hardy2005probability}.

\newpage

\subsection{Computational Causality Libraries}

A vibrant ecosystem of Python libraries has emerged over time, providing tools for various aspects of causal inference, discovery, and analysis. These libraries typically build upon foundational causal theories and aim to make causal methods accessible to data scientists, researchers, and engineers. This report summarizes several key libraries shaping the Python landscape for computational causality.

\subsubsection{DoWhy (Microsoft)}

Developed by Microsoft Research, DoWhy\footnote{https://github.com/py-why/dowhy} \cite{sharma2020dowhy} is perhaps one of the most well-known libraries aiming to provide an end-to-end workflow for causal inference. Its philosophy centers on explicitly separating the causal modeling assumptions from the statistical estimation steps, adhering to a four-stage process: 1) Modeling the causal assumptions (often using graphical models), 2) Identifying the target causal estimand based on the model, 3) Estimating the causal effect using appropriate statistical methods (like propensity scores, regression, instrumental variables), and 4) Refuting the obtained estimate through robustness checks. DoWhy aims to unify concepts from both Pearl's Structural Causal Models (SCMs) and the Potential Outcomes framework. It integrates with other libraries like EconML and CausalML for specific estimation tasks and is designed to be a general-purpose tool for applied causal analysis in data science projects.

\subsubsection{EconML (Microsoft)}

EconML\footnote{https://github.com/py-why/EconML} \cite{oprescu2019econml} focuses specifically on estimating heterogeneous treatment effects (HTE) – understanding how the effect of an intervention or treatment varies across different individuals or subgroups. It heavily leverages machine learning techniques to model complex conditional outcome expectations and propensity scores while incorporating causal identification strategies to ensure the validity of the effect estimates. Key methodologies implemented include Double Machine Learning (DML), Orthogonal Random Forests, Deep Instrumental Variables (DeepIV), and various "meta-learners" (S-learner, T-learner, X-learner) that adapt standard ML models for causal effect estimation. EconML is particularly powerful for applications in economics, personalized medicine, and targeted marketing where understanding individualized causal responses is critical.


\subsubsection{CausalML (Uber)}


Developed initially at Uber, CausalML\footnote{https://github.com/uber/causalml} \cite{zhao2023causal} is another library primarily focused on treatment effect estimation and, notably, uplift modeling. Uplift modeling specifically aims to estimate the incremental impact of an intervention on an individual's behavior – identifying who would be positively influenced by an action (e.g., receiving a promotion) compared to doing nothing. CausalML provides implementations of various uplift algorithms, including tree-based methods (causal trees/forests) and meta-learners similar to those in EconML. It's geared towards practical industry applications, especially in customer relationship management (CRM) and marketing, where optimizing interventions based on predicted individual uplift is a key objective.

\subsubsection{CausalNex (McKinsey)}

CausalNex\footnote{https://github.com/mckinsey/causalnex} takes a different approach, focusing more strongly on causal discovery and the use of Bayesian Networks for causal reasoning. It provides tools to learn causal graph structures from data, potentially incorporating domain knowledge to constrain the search space. It implements structure learning algorithms (like NOTEARS) and allows users to fit Bayesian Networks to the data based on the learned (or provided) graph structure. Once the network is built, users can perform queries (e.g., conditional probability queries, interventions via the do-calculus if the graph assumptions hold) to understand relationships and simulate scenarios within the modeled system. CausalNex is particularly useful for exploring and visualizing complex systems where understanding the network of causal influences is a primary goal.

\newpage

\subsection{Causal Inference with Large Language Models (LLMs)}

The intersection of causal inference and Large Language Models (LLMs) has emerged as a vibrant and rapidly developing research frontier. Foundational work has mapped out the potential for using LLMs as interactive causal knowledge engines, capable of answering queries about causal relationships, interventions, and counterfactuals by drawing on the vast information embedded in their training data \cite{kiciman2023causal}. This opens up the possibility of automating parts of the causal modeling process that have traditionally been highly manual.

However, a fundamental and critical question shadows this potential: whether LLMs, trained on vast quantities of observational and correlational text, can distinguish true causation from spurious association. Research specifically investigating this issue has shown that LLMs often struggle to infer causation correctly when faced with scenarios where correlation and causation are deliberately misaligned, highlighting a significant risk of the models simply repeating the statistical patterns in their training data rather than performing genuine causal reasoning \cite{jin2023can}.

To address this challenge and move beyond anecdotal evaluation, the research community has focused on creating structured and comprehensive benchmarks to systematically assess the causal reasoning capabilities of LLMs. For instance, the CLADDER framework was developed to generate complex, language-based causal problems from underlying structural causal models, allowing for a controlled and fine-grained analysis of model performance and failure modes \cite{jin2023cladder}. In a similar vein, CausalBench provides another comprehensive benchmark suite designed to evaluate a wide array of causal reasoning skills, from basic causal discovery to complex counterfactual inference \cite{wang2024causalbench}. Collectively, this body of work indicates that while LLMs show promise, they are not yet reliable causal reasoners. The current research emphasis is therefore on creating robust evaluation frameworks to rigorously map the boundaries of their capabilities and better understand their systematic weaknesses.

\subsection{Causal Inference at Industry Scale}

An open challenge for the practical application of causal inference remains scalability. In enterprise environments such as Netflix, Google, and Meta, causal questions must be answered using datasets with millions or billions of observations. Causal inference at scale has exposed a significant gap between theoretical algorithms and practical feasibility due to two known limitations:

 The first is causal discovery, where the goal is to learn the graph structure from data. Score-based search methods are generally NP-hard in the worst case \cite{Chickering2002Optimal}, and even faster constraint-based methods like the PC algorithm face a combinatorial explosion of required conditional independence tests in dense or high-dimensional graphs \cite{Kalisch2007Estimating}.
 
 The second is in causal effect estimation, especially in the presence of high-dimensional confounding. To address this, a significant body of work has emerged around Double/Debiased Machine Learning (DML) \cite{Chernozhukov2018Double}. The DML framework provides a recipe for using powerful, arbitrary machine learning models to flexibly control for a large number of confounders without introducing bias into the final treatment effect estimate. This line of research is explicitly motivated by the need to apply causal inference in settings where the number of variables makes traditional statistical methods intractable.
 
 In response to these challenges, a major engineering focus has been the development of causal inference platforms. These internal systems are designed to automate and scale causal analyses, enabling data scientists to run thousands of experiments and quasi-experiments reliably and efficiently. 
 
\newpage