\section{Limitations}
\label{sec:limitations}

While DeepCausality introduces a novel and comprehensive framework for context-aware causal reasoning, several areas represent current limitations or avenues for future development. These are acknowledged to provide a transparent understanding of the framework's present scope and capabilities.

\subsection{Counterfactual reasoning}

A primary conceptual distinction lies in its approach to counterfactual reasoning. Unlike Structural Causal Model (SCM) frameworks that employ a three-step abduction-action-prediction algorithm operating on explicit exogenous variables, DeepCausality, with its foundation in hypergeometric representations and operational Causaloids, does not currently offer a direct, equivalent implementation of full SCM-style counterfactuals. Similarly, while the ``First-Principle Methodology'' emphasizes assumption verification for model transportability, formal algorithms for causal transportability beyond these checks are an area for ongoing research in the broader field and are not yet codified within DeepCausality.

\subsection{Probabilistic reasoning}

The exploration of probabilistic reasoning within DeepCausality has been secondary to establishing the core hypergeometric context and causal structures. At present, probabilities can serve as inputs to Contextoids and Causaloids, and probabilistic calculations can be performed within a Causaloid's internal logic. However, these must ultimately resolve to a Boolean activation state. Direct output of continuous probability distributions from Causaloids is not a first-class feature, representing an implementation choice that may be revisited as methods for unifying deterministic and probabilistic outputs in a performant manner are explored.

\subsection{Causal sensitivity analysis}

Concerning causal sensitivity analysis, DeepCausality's architecture, with its explicit Context Engine and composable Causaloids, provides the conceptual hooks for such investigations. For instance, one could simulate the impact of hypothetical unobserved confounders via Contextoids or assess the effect of alternative Causaloid function specifications. However, dedicated, built-in algorithms for systematic sensitivity analysis, akin to statistical E-values or Rosenbaum bounds for unobserved confounding, are not yet implemented. The existing ``First-Principle Methodology'' for model transfer, with its emphasis on explicit assumption checking against new contexts, does offer a foundational form of sensitivity analysis regarding the transportability of causal claims, and further development of specialized sensitivity analysis tools is a recognized area for future work.

\subsection{Causal discovery}

DeepCausality in its current iteration does not support automated causal discovery. The learning of multi-layered causal relationships across hypergeometric structures, within potentially multiple, mixed-geometry (Euclidean and non-Euclidean) contexts, presents a formidable open research challenge. Consequently, all causal structures and Causaloid functions must presently be explicitly defined by the user based on domain knowledge or other discovery methods. Addressing this ``triple challenge'' of causal discovery for DeepCausality remains a significant subject for future research.

\subsection{Miscellaneous}


Regarding data input integration, DeepCausality is designed to be flexible. If input data from sources like streaming systems (e.g., Kafka) or relational database queries are directly usable within the Rust ecosystem, they can be integrated. However, highly unstructured data, such as that scraped from websites, will necessitate user-defined preprocessing, a common requirement for most analytical systems. A strength of the framework is its lack of imposition on specific data formats for contextual information; its support for non-Euclidean representations allows for the native incorporation of structured knowledge, such as standardized ontologies (e.g., ICD-10 in medicine), as an alternative or complement to vector embeddings.

From a scalability perspective, while no hard theoretical limits are imposed by the design (object IDs are 64-bit unsigned integers), practical constraints exist. Very large context graphs (e.g., exceeding 10,000 nodes with complex data types) can exhibit significant memory consumption (potentially 80-100GB), which may challenge consumer-grade hardware. On server-class infrastructure with ample memory, models with hundreds of thousands or even millions of nodes are conceivable, though performance might then become bottlenecked by CPU cache efficiency during random access patterns. Future optimizations of the underlying graph data structures may offer some mitigation, but for extremely large-scale problems, model redesign or distributed approaches might be necessary.

The initial learning curve for DeepCausality is acknowledged as steep. This arises from the confluence of Rust's own learning demands (particularly the borrow checker), the framework's novel hypergeometric representation for both context and causality, the concept of isomorphic recursive causal data structures, and its theoretical underpinnings which include non-linear time conceptualizations. This complexity is deemed a necessary corollary to the framework's design for tackling inherently complex systems requiring high reliability. It is anticipated that as adoption grows and more systems are built using DeepCausality, community contributions and the development of dedicated tooling—currently absent due to the framework's novelty—will gradually lower this barrier and improve ease of use. Integration with existing data science pipelines is facilitated by Rust's interoperability with common infrastructure like messaging systems and databases, as demonstrated by a 2024 proof-of-concept project integrating DeepCausality with a database via a messaging bus\footnote{\url{https://deepcausality.com/blog/real-time-streaming-analytics/}}.


\section{Threats to Validity}
\label{sec:threats_to_validity}

A critical assessment of DeepCausality also requires consideration of potential threats to the validity of the claims and conclusions presented in this work.

\subsection{Internal validity}

Regarding \textbf{internal validity}, the conceptual claims about features such as non-Euclidean context and recursive isomorphism are demonstrated through the provided formalism and the Rust implementation's design, which explicitly links concepts to code. The presented synthetic benchmarks offer initial evidence of performance characteristics; however, they do not fully generalize to all complex real-world scenarios. These benchmarks primarily serve as a proxy for relative performance against non-Rust frameworks, and more extensive testing with realistic, complex causal functions and graph topologies is needed for comprehensive real-world performance validation. Should an experience report be included, any selection of use cases would inherently carry some bias relative to the specific problems addressed by an organization. This is an acknowledged aspect of applied research, and transparency regarding the scope and limitations of such case studies is paramount.

\subsection{External validity}

Concerning \textbf{external validity}, the core concepts of DeepCausality are designed to be language-agnostic, though their full realization benefits from advanced type system features found in languages like Rust or Swift (the original proof-of-concept language). Replication in languages with less expressive type systems (e.g., Perl) would be substantially more challenging, if not impossible, to achieve with the same degree of safety and abstraction. The performance characteristics are significantly attributed to Rust's compiled nature and memory management; implementations in interpreted languages would be expected to be slower. The ``First-Principle Methodology'' inherently relies on the availability of significant domain expertise for hypothesis formulation and, critically, for defining the evaluation functions within Assumption objects. This reliance may limit its practical applicability in domains where such deep expertise is scarce or where causal mechanisms are poorly understood, a context in which purely data-driven causal discovery methods might be initially more appropriate. While DeepCausality supports the definition of dynamic causal structures, the practical ease with which users can define and manage the "generator functions" for these adaptive structures is an area requiring further exploration and potentially dedicated tooling. Examples of dynamic structures will be further developed and shared.


\subsection{Construct validity}

With respect to \textbf{construct validity}, the proposed "Conjoint Delta" is a custom metric for quantifying unexplained causal influence based on an operational definition of causality. While its derivation is stated, it is not a universally accepted standard metric from existing causal literature. Conventional metrics might not readily account for DeepCausality's hypergeometric and non-Euclidean representations. To address this, a more flexible approach is envisioned where causality metrics are defined via a dedicated trait, allowing users to experiment with multiple metrics, including established ones adapted for this framework, to determine which best suit specific problem domains within DeepCausality. The "explainability" provided by DeepCausality is achieved through recursive functional traversal of causal paths, yielding a trace of activated Causaloids and the data values influencing their decisions. While this offers a high degree of transparency into the model's internal logic, a detailed comparative analysis against the diverse spectrum of existing XAI methodologies, especially those not designed for hypergraphs or non-Euclidean spaces, is a subject for future elaboration.

\subsection{Conclusion validity}

Finally, regarding \textbf{conclusion validity and interpretive issues}, alternative interpretations of DeepCausality's conceptual advantages are possible. One might argue that the rapid progress of LLMs obviates the need for more complex, structured causal frameworks. However, this view arguably overlooks the complementary nature of these approaches; DeepCausality aims to provide a rigorous causal reasoning engine that could synergize with the perceptual and generative capabilities of LLMs, addressing their known limitations in mechanistic understanding and robust reasoning. Another perspective might be that the introduction of hypergraphs and non-Euclidean spaces adds unnecessary complexity. While this is true for simpler problem domains where DeepCausality would indeed be an over-engineered solution, for notoriously hard problems involving intricate, multi-way interactions or abstract relational structures, these advanced representations offer a novel and potentially more faithful way to model reality. The inherent complexity and steep learning curve of DeepCausality are acknowledged; its design is aimed at problems where simpler methods have proven insufficient. The comparisons to other causal paradigms in this work are intended to respectfully situate DeepCausality upon the foundations they have laid, highlighting its unique contributions rather than offering a critique of their strengths in their respective domains. The critical assessment of correlation-based methodologies stems from their documented theoretical limitations (e.g., IID assumption, universal approximation theorem constraints) when applied to causal inference.