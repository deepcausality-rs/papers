\section{Motivation}
\label{sec:motivation}

Recent advancements in artificial intelligence, particularly in deep learning (DL) and large language models (LLMs), have demonstrated remarkable proficiency in processing vast amounts of data, recognizing intricate patterns, and generating human-like text and responses \cite{lecun2015deep, schmidhuber2015deep, goodfellow2016Deep}. These systems can summarize information, answer complex questions, and even generate creative content, leading to their rapid adoption across numerous fields. However, alongside these successes, a closer examination reveals inherent limitations stemming from their primarily correlation-based foundations.

\subsection{Limitations of corelation based methods}
\label{subsec:limits}   

The impressive achievements of deep learning models are undeniable, having revolutionized fields from natural language processing to computer vision \cite{lecun2015deep, schmidhuber2015deep,goodfellow2016Deep, Scholkopf2021Toward}. However, these successes are largely predicated on their strength as statistical techniques capable of sophisticated pattern learning or ``curve fitting'' over vast datasets \cite{pearl2018theoretical, Scholkopf2021Toward}. This foundation gives rise to several inherent limitations, hindering their ability to achieve genuine understanding and robust, trustworthy reasoning, particularly in complex, dynamic, and high-stakes environments \cite{pearl2018theoretical, Scholkopf2021Toward, Kaddour2022Causal}. These limitations include significant challenges in generalizing beyond the training data space and handling distribution shifts \cite{Scholkopf2021Toward}, vulnerability to subtle input perturbations \cite{Scholkopf2021Toward}, a lack of transparency and interpretability which is critical in high-stakes applications \cite{Scholkopf2021Toward}, and a fundamental inability to inherently reason about interventions or causality \cite{pearl2018theoretical, Scholkopf2021Toward, Kaddour2022Causal}.

\textbf{Correlation does not imply Causation}: Deep learning models excel at identifying and exploiting statistical correlations within data \cite{Scholkopf2021Toward, Kaddour2022Causal, Bareinboim2012causal, Guo2018Survey}. However, as famously articulated by Judea Pearl \cite{Pearl2009Causality}, these achievements are largely based on sophisticated "curve fitting" or pattern interpolation over vast datasets\cite{Pearl2009Causality, Bareinboim2020On, Scholkopf2021Toward}. They do not inherently distinguish between correlation and causation \cite{Pearl2009Causality, Bareinboim2012causal, Scholkopf2021Toward, Kaddour2022Causal, wolff2018solutions, Peters2017Elements, Guo2018Survey, Guyon2010Causality}. Machine learning applied to observational data \cite{Peters2017Elements}, where predictive variables are not under the learner's control \cite{Bareinboim2012causal}, can typically only learn correlations \cite{Bareinboim2012causal, Peters2017Elements}. Consequently, a model might learn that factor A is highly correlated with outcome B \cite{Bareinboim2012causal}, but it cannot, without explicit causal information or intervention capabilities \cite{Pearl2009Causality, Bareinboim2012causal, Scholkopf2021Toward, Kaddour2022Causal, Peters2017Elements, Lattimore2016Causal, Lee2018Structural}, determine if A causes B, B causes A, or if a hidden common cause C influences both \cite{Bareinboim2012causal, Peters2017Elements}. This limitation is critical for decision-making \cite{Bareinboim2012causal, Kaddour2022Causal, Wang2020Causal, Lu2020Regret, Aglietti2020Causal}, as acting upon a spurious correlation \cite{Bareinboim2012causal, Peters2017Elements, Lu2020Regret} can lead to ineffective or even detrimental interventions \cite{Bareinboim2012causal, Lu2020Regret}.

\textbf{The Independent and Identically Distributed (IID) Data Assumption:} Many foundational machine learning algorithms, and by extension some deep learning approaches, assume that data points are drawn independently from the same underlying distribution \cite{jordan2015machine}. 
This assumption is frequently violated in real-world scenarios \cite{Scholkopf2021Toward, Kohavi2009Controlled}. Financial market data exhibits temporal dependencies and regime shifts \cite{ Pena2016Multivariate}; social network data is characterized by complex interdependencies \cite{baykal2022bandit, Wang2020Causal}; industrial sensor data may be autocorrelated \cite{binkowski2017autoregressive, Li2020Enhanced, Runge2019Detecting}. When the IID assumption breaks down, the performance and reliability of DL models can degrade significantly \cite{Scholkopf2021Toward}, as the patterns learned during training may no longer hold in production \cite{Scholkopf2021Toward}. DARPA itself has noted the limitations of treating "each data set as an independent, uncorrelated input" and the need to model "underlying causal mechanism[s]" \cite{DARPA_ANSR}.

\textbf{The "Black Box" Problem and Lack of Genuine Explainability}: Deep neural networks, especially those with billions or trillions of parameters like modern LLMs, are often described as "black boxes". \cite{Scholkopf2021Toward, Kaddour2022Causal}. While techniques from Explainable AI (XAI) can provide post-hoc rationalizations or highlight influential features, they often do not reveal the true internal computational path or the underlying reasoning process in a verifiable way \cite{Scholkopf2021Toward, Kaddour2022Causal, wolff2018solutions}. Deep learning models learn statistical patterns and associations, but may not capture underlying causal mechanisms \cite{Scholkopf2021Toward}. The opacity makes it difficult to debug models \cite{Moraffah2020Causal}, and is a potential liability when using deep learning for problem domains medical diagnosis, where human users might like to understand how a system made a decision \cite{Moraffah2020Causal}. Such opacity can also lead to issues of bias \cite{Moraffah2020Causal}. Model explanations allow us to argue for model decisions and exhibit the situation when algorithmic decisions might be biased or discriminating, and precise explanations may facilitate model debugging and error analysis \cite{Moraffah2020Causal}. It is important to build trust, particularly in critical applications \cite{Moraffah2020Causal}.

\textbf{Knowledge Cut-off and Real-Time Relevance: }LLMs are trained on data up to a specific point in time. Consequently, they lack knowledge of events, discoveries, or shifts in the world that occur after their training data concluded. While "search grounding" – augmenting LLMs with real-time internet search – provides access to current facts, it does not inherently equip the model with an understanding of evolving dynamics or the ability to integrate new information into a coherent, evolving causal model of a system's state \cite{Scholkopf2021Toward}. The interpretation of new information is still performed through the lens of patterns learned from historical data \cite{Scholkopf2021Toward}.


\textbf{Lack of Innate Conceptualization of Time, Space, and Causality: } While deep learning models can learn patterns related to these concepts from data, they do not possess them as foundational, structured elements of their reasoning framework in the way humans do \cite{Davis2015Commonsense, Marcus2018Deep, Pearl2009Causality, Scholkopf2021Toward}. Rather, true AI requires an innate grasp of fundamental concepts like time, space, and causality \cite{Pearl2009Causality, Scholkopf2021Toward, Kaddour2022Causal, bishop2020artificialintelligencestupidcausal}.

\textbf{Constraints of Euclidean Data Representation:} Many deep techniques rely on embedding data into Euclidean vector spaces. However, as J.M. Bishop highlights, relational structures (e.g., social networks, molecular interactions, conceptual hierarchies) are often better represented using non-Euclidean geometries like graphs or hypergraphs. Forcing such data into Euclidean spaces can lead to a loss of information or the introduction of artificial relationships. \cite{Ouvrard2020Hypergraphs, bishop2020artificialintelligencestupidcausal}. 

These limitations underscore the need for AI paradigms that go beyond statistical pattern matching to incorporate explicit causal understanding, robust contextual awareness, and transparent, verifiable reasoning. DeepCausality proposes as a framework designed to contribute to this endeavor.


\subsection{Consequences of Over-Reliance on Correlational}
\label{subsec:consequences_of_limitations}

The identified limitations of primarily correlation-based AI systems are not merely academic concerns; they precipitate significant and often detrimental consequences, both immediate and higher-order, when these systems are deployed in the real world. An over-reliance on statistical pattern matching without deeper causal understanding can lead to:


    \textbf{Failure in Novel or Dynamic Environments:} Systems trained on specific data distributions (violating the IID assumption in practice) often fail catastrophically when encountering out-of-distribution data or shifts in the underlying generative process \cite{Scholkopf2021Toward}. This brittleness makes them unreliable for applications in evolving settings, such as financial markets or autonomous navigation, where adaptability is paramount. The inability to distinguish causation from spurious correlation means models may latch onto ephemeral patterns that do not generalize.

    \textbf{Lack of Trust and Accountability:} The "black box" nature of many deep learning models, where the reasoning pathway from input to output is inscrutable, severely erodes trust, especially in high-stakes decision-making like medical diagnosis or legal applications \cite{Moraffah2020Causal, Kaddour2022Causal}. Without genuine explainability, it is difficult to debug failures, assign responsibility for erroneous or biased outcomes, or assure users and regulators of the system's safety and fairness.

    \textbf{Ineffective or Harmful Interventions:} If an AI system recommends an action based on a learned correlation that is not truly causal, the resulting intervention may be ineffective at best, or actively harmful at worst \cite{Bareinboim2012causal, Peters2017Elements}. For example, a system might correlate a symptom with a recovery outcome without understanding that both are caused by an underlying treatment, leading to potentially flawed recommendations if it tries to manipulate the symptom directly.

     \textbf{Stagnation in Scientific Discovery and Understanding:} Progress in many scientific fields relies on uncovering underlying causal mechanisms, not just predicting outcomes. AI systems that cannot move beyond correlation to provide insights into these mechanisms offer limited value for advancing fundamental scientific understanding or generating truly novel, mechanistically sound hypotheses.

     \textbf{Ethical Concerns and Perpetuation of Bias:} Models learning from biased historical data without an understanding of the causal factors generating those biases are prone to perpetuating or even amplifying societal inequities \cite{Moraffah2020Causal}. A causal perspective is often necessary to identify and mitigate such biases by understanding their origins.

    \textbf{Limited Adaptability and Continuous Learning:} Systems lacking innate conceptualizations of fundamental constructs like time, space, and causality, or those constrained by fixed knowledge cut-offs, struggle with genuine continuous learning and adaptation. They cannot easily integrate new information into a coherent, evolving model of the world's underlying causal structure.

These consequences, ranging from practical failures in dynamic applications to fundamental limitations in achieving trustworthy and generalizable intelligence, collectively highlight a profound need for AI paradigms that can transcend the mere fitting of surface-level statistical associations. While pattern recognition has propelled AI to remarkable achievements, the increasing deployment of these systems in complex, high-stakes, and human-facing domains demands a move towards architectures built upon a more robust foundation of causal understanding and explicit mechanistic reasoning. Without this shift, AI systems will likely continue to struggle with out-of-distribution generalization, remain opaque in their decision-making processes, risk perpetuating harmful biases, and prove inadequate for tasks requiring true comprehension of how and why events unfold or how interventions will genuinely alter outcomes. Therefore, it is time for the development of new foundational frameworks capable of imbuing intelligent systems with a deeper, more principled, and ultimately more reliable grasp of the causal fabric of the world.

\newpage

\subsection{The Path Forward: Principles for Causally-Grounded Intelligence}
\label{subsec:path_forward_principles}

These pervasive challenges and their significant consequences underscore the necessity for computational frameworks that move beyond purely statistical pattern matching and embrace principles of explicit causal modeling. We argue that a path forward towards more robust, trustworthy, and adaptable AI requires systems capable of adhering to several core philosophical tenets. Each of these tenets aims to directly mitigate one or more of the limitations and adverse effects identified above:

\textbf{Explicitly representing causal mechanisms rather than just learning correlations:} This directly counters the "correlation does not imply causation" problem, aiming to build models that reflect underlying generative processes. This provides a foundation for more reliable predictions, especially in novel situations, and enables more effective interventions by targeting true causes rather than mere symptoms or associated variables.

     \textbf{Deeply integrating rich, multi-dimensional, and dynamic context (including Euclidean and non-Euclidean relations) into the reasoning process:} This addresses the failures arising from the IID assumption and the knowledge cut-off. By making models acutely aware of their operational environment and its evolution—across various data types, temporal scales, and spatial configurations—their adaptability and relevance in dynamic settings are significantly enhanced.

    \textbf{Making foundational assumptions transparent, verifiable, and integral to model validity and transfer:} This tackles the "black box" problem and issues of trust. When assumptions are explicit and, where possible, testable (even if only against contextual consistency), the conditions under which a model's causal claims hold become clearer, facilitating safer deployment and more principled model transportability to new domains.

    \textbf{Employing structured, composable representations for causal knowledge that allow for transparency and scalability:} This further aids explainability and helps manage the complexity of real-world causal systems. Modular, hierarchical models are easier to understand, debug, and extend, countering the opacity of monolithic neural networks.

    \textbf{Leveraging implementation choices that prioritize performance, safety, and reliability for practical application:} This ensures that sophisticated causal reasoning capabilities can be deployed effectively in real-world systems, including those with demanding performance requirements or safety-critical functions.

It is crucial to recognize that the path forward is not necessarily a binary choice between correlation-based deep learning and explicit causal modeling. Rather, the most profound advancements in AI are likely to emerge from the synergy of both paradigms fusion together. Large-scale deep learning models excel at perceptual tasks and extracting complex patterns from vast, unstructured data. Structured causal frameworks can then leverage these extracted features and initial hypotheses, embedding them within a rigorous system of mechanistic reasoning, contextual understanding, and verifiable assumptions. This combination allows for the creation of intelligent systems that are both broadly aware and deeply analytical, capable of learning from data while understanding the causal fabric of the world they operate in. The remainder of this preprint introduces DeepCausality, a novel framework designed around these synergistic principles, aiming to provide a concrete pathway for developing such causally-grounded and ultimately more intelligent systems.